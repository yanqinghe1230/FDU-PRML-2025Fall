{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手动实现 Softmax 与参数调优\n",
    "\n",
    "## 学习目标\n",
    "1. **手动实现 Softmax** - 深入理解 softmax 的数学原理和数值稳定性\n",
    "2. **参数调优实验** - 系统地调整超参数以提升模型性能\n",
    "3. **实验对比分析** - 学会科学地对比不同配置的效果\n",
    "\n",
    "## 实验流程\n",
    "- Part 1: 手动实现 Softmax 函数\n",
    "- Part 2: 使用自定义 Softmax 构建模型\n",
    "- Part 3: 参数调优实验\n",
    "- Part 4: 结果分析与可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "    \n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 手动实现 Softmax\n",
    "\n",
    "### 任务1.1：理论复习\n",
    "\n",
    "Softmax 函数定义：\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "**数值稳定性技巧**：为了避免指数运算溢出，通常会减去最大值：\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i - \\max(z)}}{\\sum_{j=1}^{K} e^{z_j - \\max(z)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 任务1.1 - 手动实现 Softmax 函数\n",
    "def my_softmax(logits):\n",
    "    \"\"\"\n",
    "    手动实现 softmax 函数，需要考虑数值稳定性\n",
    "    \n",
    "    Args:\n",
    "        logits: torch.Tensor, shape [batch_size, num_classes]\n",
    "    \n",
    "    Returns:\n",
    "        probs: torch.Tensor, shape [batch_size, num_classes]\n",
    "        \n",
    "    提示：\n",
    "    1. 使用 logits.max(dim=-1, keepdim=True) 找到每行的最大值\n",
    "    2. 减去最大值后再计算 exp\n",
    "    3. 除以行和进行归一化\n",
    "    \"\"\"\n",
    "    # ========== 开始编写代码 ==========\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ========== 结束编写代码 ==========\n",
    "    pass\n",
    "\n",
    "# 测试你的实现\n",
    "def test_my_softmax():\n",
    "    print(\"测试 my_softmax 函数\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 测试用例1：简单情况\n",
    "    logits1 = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "    my_result1 = my_softmax(logits1)\n",
    "    torch_result1 = F.softmax(logits1, dim=-1)\n",
    "    \n",
    "    print(\"测试用例1 (简单)：\")\n",
    "    print(f\"输入: {logits1}\")\n",
    "    print(f\"你的输出: {my_result1}\")\n",
    "    print(f\"PyTorch: {torch_result1}\")\n",
    "    print(f\"是否一致: {torch.allclose(my_result1, torch_result1, rtol=1e-5)}\")\n",
    "    print(f\"输出和为1: {torch.allclose(my_result1.sum(dim=-1), torch.tensor([1.0]))}\")\n",
    "    print()\n",
    "    \n",
    "    # 测试用例2：batch 情况\n",
    "    logits2 = torch.tensor([[1.0, 2.0, 3.0], [0.5, 1.5, 2.5]])\n",
    "    my_result2 = my_softmax(logits2)\n",
    "    torch_result2 = F.softmax(logits2, dim=-1)\n",
    "    \n",
    "    print(\"测试用例2 (batch)：\")\n",
    "    print(f\"是否一致: {torch.allclose(my_result2, torch_result2, rtol=1e-5)}\")\n",
    "    print()\n",
    "    \n",
    "    # 测试用例3：极端值（数值稳定性）\n",
    "    logits3 = torch.tensor([[100.0, 200.0, 300.0]])\n",
    "    try:\n",
    "        my_result3 = my_softmax(logits3)\n",
    "        torch_result3 = F.softmax(logits3, dim=-1)\n",
    "        print(\"测试用例3 (大数值 - 数值稳定性)：\")\n",
    "        print(f\"是否一致: {torch.allclose(my_result3, torch_result3, rtol=1e-5)}\")\n",
    "        print(f\"是否有 NaN: {torch.isnan(my_result3).any()}\")\n",
    "        print(f\"是否有 Inf: {torch.isinf(my_result3).any()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"测试用例3 失败: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# 运行测试\n",
    "test_my_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务1.2：创建自定义 Softmax 层\n",
    "\n",
    "将你实现的 softmax 函数封装成一个 PyTorch 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 任务1.2 - 创建自定义 Softmax 层\n",
    "class MySoftmax(nn.Module):\n",
    "    \"\"\"\n",
    "    自定义 Softmax 层，使用我们手动实现的 softmax 函数\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ========== 开始编写代码 ==========\n",
    "        # 调用 my_softmax 函数\n",
    "        \n",
    "        \n",
    "        # ========== 结束编写代码 ==========\n",
    "        pass\n",
    "\n",
    "# 测试自定义层\n",
    "test_layer = MySoftmax()\n",
    "test_input = torch.randn(2, 10)\n",
    "output = test_layer(test_input)\n",
    "print(f\"输出shape: {output.shape}\")\n",
    "print(f\"每行和为1: {torch.allclose(output.sum(dim=-1), torch.ones(2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Fashion-MNIST 数据集\n",
    "train_ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {len(train_ds)}\")\n",
    "print(f\"测试集大小: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 构建带有自定义 Softmax 的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准模型：输出 logits\n",
    "class StandardModel(nn.Module):\n",
    "    def __init__(self, hidden1=300, hidden2=100):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 任务2.1 - 创建带有自定义 Softmax 的模型\n",
    "class CustomSoftmaxModel(nn.Module):\n",
    "    \"\"\"\n",
    "    使用自定义 Softmax 层的模型\n",
    "    注意：这个模型输出概率，不是 logits\n",
    "    因此需要配合 NLLLoss 使用\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden1=300, hidden2=100):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # ========== 开始编写代码 ==========\n",
    "        # 定义网络结构，最后一层使用 MySoftmax\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ========== 结束编写代码 ==========\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ========== 开始编写代码 ==========\n",
    "        \n",
    "        \n",
    "        # ========== 结束编写代码 ==========\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数：评估、回调等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluating(model, dataloader, loss_fct):\n",
    "    \"\"\"评估函数\"\"\"\n",
    "    loss_list = []\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for datas, labels in dataloader:\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(datas)\n",
    "        loss = loss_fct(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        preds = outputs.argmax(axis=-1)\n",
    "        pred_list.extend(preds.cpu().numpy().tolist())\n",
    "        label_list.extend(labels.cpu().numpy().tolist())\n",
    "    \n",
    "    acc = accuracy_score(label_list, pred_list)\n",
    "    return np.mean(loss_list), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class TensorBoardCallback:\n",
    "    \"\"\"TensorBoard 可视化回调\"\"\"\n",
    "    def __init__(self, log_dir, flush_secs=10):\n",
    "        self.writer = SummaryWriter(log_dir=log_dir, flush_secs=flush_secs)\n",
    "\n",
    "    def add_scalars(self, step, **kwargs):\n",
    "        if 'loss' in kwargs and 'val_loss' in kwargs:\n",
    "            self.writer.add_scalars(\n",
    "                \"training/loss\",\n",
    "                {\"train\": kwargs['loss'], \"val\": kwargs['val_loss']},\n",
    "                global_step=step\n",
    "            )\n",
    "        if 'acc' in kwargs and 'val_acc' in kwargs:\n",
    "            self.writer.add_scalars(\n",
    "                \"training/accuracy\",\n",
    "                {\"train\": kwargs['acc'], \"val\": kwargs['val_acc']},\n",
    "                global_step=step\n",
    "            )\n",
    "        if 'lr' in kwargs:\n",
    "            self.writer.add_scalar(\"training/learning_rate\", kwargs['lr'], global_step=step)\n",
    "    \n",
    "    def __call__(self, step, **kwargs):\n",
    "        self.add_scalars(step, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopCallback:\n",
    "    \"\"\"早停回调\"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = -1\n",
    "        self.counter = 0\n",
    "        \n",
    "    def __call__(self, metric):\n",
    "        if metric >= self.best_metric + self.min_delta:\n",
    "            self.best_metric = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "    \n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    tensorboard_callback=None,\n",
    "    early_stop_callback=None,\n",
    "    eval_step=500,\n",
    "):\n",
    "    \"\"\"训练函数\"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'steps': []\n",
    "    }\n",
    "    \n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(total=epochs * len(train_loader), desc=\"Training\") as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            for datas, labels in train_loader:\n",
    "                datas = datas.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(datas)\n",
    "                loss = loss_fct(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 评估\n",
    "                if global_step % eval_step == 0:\n",
    "                    preds = outputs.argmax(axis=-1)\n",
    "                    train_acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "                    \n",
    "                    model.eval()\n",
    "                    val_loss, val_acc = evaluating(model, val_loader, loss_fct)\n",
    "                    model.train()\n",
    "                    \n",
    "                    history['train_loss'].append(loss.item())\n",
    "                    history['train_acc'].append(train_acc)\n",
    "                    history['val_loss'].append(val_loss)\n",
    "                    history['val_acc'].append(val_acc)\n",
    "                    history['steps'].append(global_step)\n",
    "                    \n",
    "                    if tensorboard_callback:\n",
    "                        tensorboard_callback(\n",
    "                            global_step,\n",
    "                            loss=loss.item(),\n",
    "                            val_loss=val_loss,\n",
    "                            acc=train_acc,\n",
    "                            val_acc=val_acc,\n",
    "                            lr=optimizer.param_groups[0]['lr']\n",
    "                        )\n",
    "                    \n",
    "                    if early_stop_callback:\n",
    "                        early_stop_callback(val_acc)\n",
    "                        if early_stop_callback.early_stop:\n",
    "                            print(f\"\\n早停于 epoch {epoch}, step {global_step}\")\n",
    "                            return history\n",
    "                \n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'epoch': epoch, 'val_acc': history['val_acc'][-1] if history['val_acc'] else 0})\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 参数调优实验\n",
    "\n",
    "### 实验设计\n",
    "我们将对比以下超参数的影响：\n",
    "1. **学习率** (lr): 0.0001, 0.001, 0.01, 0.1\n",
    "2. **批次大小** (batch_size): 16, 32, 64, 128\n",
    "3. **网络结构** (hidden_size): (128, 64), (256, 128), (512, 256)\n",
    "4. **优化器**: SGD, Adam\n",
    "4. **探索**: 鼓励使用其他不同参数探索最优模型\n",
    "\n",
    "### 任务3.1：学习率调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 任务3.1 - 学习率调优实验\n",
    "def experiment_learning_rate():\n",
    "    \"\"\"\n",
    "    实验：对比不同学习率的效果\n",
    "    固定其他参数：batch_size=64, hidden=(256, 128), optimizer=SGD\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"实验1：学习率调优\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "    results = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\n训练 lr={lr}...\")\n",
    "        \n",
    "        # ========== 开始编写代码 ==========\n",
    "        # 1. 创建数据加载器 (batch_size=64)\n",
    "        \n",
    "        \n",
    "        # 2. 创建模型 (hidden1=256, hidden2=128)\n",
    "        \n",
    "        \n",
    "        # 3. 定义损失函数和优化器\n",
    "        \n",
    "        \n",
    "        # 4. 创建回调函数\n",
    "        \n",
    "        \n",
    "        # 5. 训练模型 (epochs=10)\n",
    "        \n",
    "        \n",
    "        # 6. 保存结果\n",
    "        # results[lr] = history\n",
    "        # ========== 结束编写代码 ==========\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行实验\n",
    "# lr_results = experiment_learning_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务3.2：批次大小调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 任务3.2 - 批次大小调优实验\n",
    "def experiment_batch_size():\n",
    "    \"\"\"\n",
    "    实验：对比不同批次大小的效果\n",
    "    固定其他参数：lr=0.01, hidden=(256, 128), optimizer=SGD\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"实验2：批次大小调优\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    batch_sizes = [16, 32, 64, 128]\n",
    "    results = {}\n",
    "    \n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n训练 batch_size={bs}...\")\n",
    "        \n",
    "        # ========== 开始编写代码 ==========\n",
    "        # 实现批次大小实验\n",
    "        # 提示：主要改变 DataLoader 的 batch_size 参数\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ========== 结束编写代码 ==========\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行实验\n",
    "# bs_results = experiment_batch_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务3.3：网络结构调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 任务3.3 - 网络结构调优实验\n",
    "def experiment_architecture():\n",
    "    \"\"\"\n",
    "    实验：对比不同网络结构的效果\n",
    "    固定其他参数：lr=0.01, batch_size=64, optimizer=SGD\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"实验3：网络结构调优\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    architectures = [\n",
    "        (128, 64, \"小网络\"),\n",
    "        (256, 128, \"中网络\"),\n",
    "        (512, 256, \"大网络\")\n",
    "    ]\n",
    "    results = {}\n",
    "    \n",
    "    for hidden1, hidden2, name in architectures:\n",
    "        print(f\"\\n训练 {name} (hidden1={hidden1}, hidden2={hidden2})...\")\n",
    "        \n",
    "        # ========== 开始编写代码 ==========\n",
    "        # 实现网络结构实验\n",
    "        # 提示：改变 StandardModel 的 hidden1 和 hidden2 参数\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ========== 结束编写代码 ==========\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行实验\n",
    "# arch_results = experiment_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务3.4：优化器对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 任务3.4 - 优化器对比实验\n",
    "def experiment_optimizer():\n",
    "    \"\"\"\n",
    "    实验：对比 SGD 和 Adam 优化器\n",
    "    固定其他参数：lr=0.001, batch_size=64, hidden=(256, 128)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"实验4：优化器对比\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    optimizers = ['SGD', 'Adam']\n",
    "    results = {}\n",
    "    \n",
    "    for opt_name in optimizers:\n",
    "        print(f\"\\n训练使用 {opt_name} 优化器...\")\n",
    "        \n",
    "        # ========== 开始编写代码 ==========\n",
    "        # 实现优化器对比实验\n",
    "        # 提示：\n",
    "        # if opt_name == 'SGD':\n",
    "        #     optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        # else:\n",
    "        #     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ========== 结束编写代码 ==========\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行实验\n",
    "# opt_results = experiment_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 结果分析与可视化\n",
    "\n",
    "### 绘制对比图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiment_results(results_dict, experiment_name, param_name):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    for param_value, history in results_dict.items():\n",
    "        # 绘制验证损失\n",
    "        axes[0].plot(history['steps'], history['val_loss'], \n",
    "                    label=f'{param_name}={param_value}', marker='o')\n",
    "        # 绘制验证准确率\n",
    "        axes[1].plot(history['steps'], history['val_acc'], \n",
    "                    label=f'{param_name}={param_value}', marker='o')\n",
    "    \n",
    "    axes[0].set_xlabel('Steps')\n",
    "    axes[0].set_ylabel('Validation Loss')\n",
    "    axes[0].set_title(f'{experiment_name} - Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    axes[1].set_xlabel('Steps')\n",
    "    axes[1].set_ylabel('Validation Accuracy')\n",
    "    axes[1].set_title(f'{experiment_name} - Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{experiment_name.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 使用示例\n",
    "# plot_experiment_results(lr_results, \"Learning Rate Comparison\", \"lr\")\n",
    "# plot_experiment_results(bs_results, \"Batch Size Comparison\", \"batch_size\")\n",
    "# plot_experiment_results(arch_results, \"Architecture Comparison\", \"architecture\")\n",
    "# plot_experiment_results(opt_results, \"Optimizer Comparison\", \"optimizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 汇总最佳结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(all_experiments):\n",
    "    summary = []\n",
    "    \n",
    "    for exp_name, results in all_experiments.items():\n",
    "        for param_value, history in results.items():\n",
    "            best_acc = max(history['val_acc']) if history['val_acc'] else 0\n",
    "            final_acc = history['val_acc'][-1] if history['val_acc'] else 0\n",
    "            final_loss = history['val_loss'][-1] if history['val_loss'] else float('inf')\n",
    "            \n",
    "            summary.append({\n",
    "                'Experiment': exp_name,\n",
    "                'Parameter': str(param_value),\n",
    "                'Best Val Acc': f\"{best_acc:.4f}\",\n",
    "                'Final Val Acc': f\"{final_acc:.4f}\",\n",
    "                'Final Val Loss': f\"{final_loss:.4f}\"\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(summary)\n",
    "    return df\n",
    "\n",
    "# 使用示例\n",
    "# all_results = {\n",
    "#     'Learning Rate': lr_results,\n",
    "#     'Batch Size': bs_results,\n",
    "#     'Architecture': arch_results,\n",
    "#     'Optimizer': opt_results\n",
    "# }\n",
    "# summary_df = summarize_results(all_results)\n",
    "# print(\"\\n实验结果汇总:\")\n",
    "# print(summary_df.to_string())\n",
    "# summary_df.to_csv('experiment_summary.csv', index=False)\n",
    "# print(\"\\n结果已保存到 experiment_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练最佳模型\n",
    "\n",
    "根据调优实验的结果，使用最佳超参数配置训练最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"训练最佳模型\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 根据实验结果选择的最佳参数（示例）\n",
    "    best_lr = 0.01\n",
    "    best_batch_size = 64\n",
    "    best_hidden1 = 256\n",
    "    best_hidden2 = 128\n",
    "    best_optimizer = 'Adam'\n",
    "    \n",
    "    print(f\"\\n最佳配置:\")\n",
    "    print(f\"  学习率: {best_lr}\")\n",
    "    print(f\"  批次大小: {best_batch_size}\")\n",
    "    print(f\"  网络结构: ({best_hidden1}, {best_hidden2})\")\n",
    "    print(f\"  优化器: {best_optimizer}\")\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=best_batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(test_ds, batch_size=best_batch_size, shuffle=False)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = StandardModel(hidden1=best_hidden1, hidden2=best_hidden2).to(device)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    if best_optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=best_lr, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "    \n",
    "    # 创建回调\n",
    "    tb_callback = TensorBoardCallback(\"runs/best_model\")\n",
    "    es_callback = EarlyStopCallback(patience=10)\n",
    "    \n",
    "    # 训练更多epochs\n",
    "    history = training(\n",
    "        model, train_loader, val_loader, epochs=30,\n",
    "        loss_fct=loss_fct, optimizer=optimizer,\n",
    "        tensorboard_callback=tb_callback,\n",
    "        early_stop_callback=es_callback,\n",
    "        eval_step=500\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n最终验证准确率: {max(history['val_acc']):.4f}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print(\"模型已保存到 best_model.pth\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# 训练最佳模型\n",
    "# best_model, best_history = train_best_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
