# **模式识别与机器学习 -- 实验2**

本实验包含以下部分：

-   softmax （50%） 

-   svm （50%）


## **softmax**

1 手动实现 Softmax 函数 （15%）
```
代码
```
说明

2 创建自定义 Softmax 层  （15%）
```
代码
```
说明

3 参数调优实验（无需给出代码）

4 提交实验结果，只需截图最后的实验结果汇总和最佳模型的配置即可（20%）



## **svm**

### 一、损失和梯度的计算
#### 1, 循环实现中的梯度计算（10%）

补全 fduml.linear_svm import中的svm_loss_naive函数

在这里写代码，并简单说明（注意，没有说明会适当扣分）

```
代码
```
说明

#### 2, 向量实现中的损失计算和梯度计算（15%）

补全 fduml.linear_svm import中的svm_loss_vectorized函数

在这里写代码，并简单说明

```
代码
```
说明

#### 3, 在这里提交ipynb中的相关检查结果（不占额外分数，但这是判断上面的实现是否正确的重要依据



### 二、实现SGD 
代码+简单说明（10%）

```
代码
```
说明

### 三、利用验证集做超参数调优 
表格记录（5*5），可以自己尝试不同的组合 （10%）

| 学习率 \ 正则化强度 | 0.001       | 0.01        | 0.1         | 1.0         | 10.0        |
| :----------------- | :---------- | :---------- | :---------- | :---------- | :---------- |
| **0.0001**         | {结果}      | {结果}      | {结果}      | {结果}      | {结果}      |
| **0.001**          | {结果}      | {结果}      | {结果}      | {结果}      | {结果}      |
| **0.01**           | {结果}      | {结果}      | {结果}      | {结果}      | {结果}      |
| **0.1**            | {结果}      | {结果}      | {结果}      | {结果}      | {结果}      |
| **1.0**            | {结果}      | {结果}      | {结果}      | {结果}      | {结果}      |

最优的结果的loss曲线截图和正确率截图（5%）



